{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fizz Buzz with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "this is a very simple task often chosen by an interviewer to get an idea of a candidate's ability to write simple functions. We will break it with a very simple feed forward neural network composed by a total of 3 weighted layers (2 hidden and 1 output layer).\n",
    "\n",
    "This task usually consists of writing a function that takes an integer and returns the string 'fizz' if the number is divisible by (is a multiple of) 3, 'buzz' if the number is divisible by 5, 'fizzbuzz' if the number is divisible by 3*5=15 and returning the number itself otherwise.\n",
    "\n",
    "We'll approach this task by first converting the decimal integer numbers to binary inputs, so our model will have `num_bits` values per sample and will output 4 values, corresponding to the possible classes for each sample (fizz, buzz, fizzbuzz, x).\n",
    "\n",
    "So we will start by writing the **fizz_buzz_encode** method, and other two convenience methods for encoding/decoding binary and fizz buzz, obviously after importing the usual modules, and defining the number of possible digits for representing the numbers (bits). We will set it to 12.\n",
    "\n",
    "source:\n",
    "http://joelgrus.com/2016/05/23/fizz-buzz-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# functional module, fuctional implementations for \n",
    "# unparameterized neural network modules\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "NUM_DIGITS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "write our solution and convenience methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Represent each input by an array of its binary digits.\n",
    "def binary_encode(i, num_digits):\n",
    "    return np.array([i >> d & 1 for d in range(num_digits)])\n",
    "\n",
    "# One-hot encode the desired outputs: [number, \"fizz\", \"buzz\", \"fizzbuzz\"]\n",
    "def fizz_buzz_encode(i):\n",
    "    if   i % 15 == 0: return 3\n",
    "    elif i % 5  == 0: return 2\n",
    "    elif i % 3  == 0: return 1\n",
    "    else:             return 0\n",
    "\n",
    "#printable and coherent labels\n",
    "def fizz_buzz_decode(i, prediction):\n",
    "    return [str(i), \"fizz\", \"buzz\", \"fizzbuzz\"][prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now I will show you a way to build a custom dataset class, that subclasses data.Dataset. \n",
    "\n",
    "That's a simple way to create a sort of singleton object for our data, so that if and when we instantiate the dataset object multiple times, the data doesn't get created/loaded multiple times. To do this, we create an empty dictionary **DATA_CACHE** at global scope (it will be created when we import the module). Then when we instantiate the dataset object and its init method gets called, we first check if our **DATA_CACHE** actually contains the data, if not we fill the cache with our data, otherwise we simply compute the data at each index (we need to split in train/val/test). This way we could load even a gazillion samples without actually copying data and thus using more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "DATA_CACHE = {} \n",
    "\n",
    "def fill_cache(num_bits):\n",
    "    DATA_CACHE.update({\n",
    "        'X': [binary_encode(i, NUM_DIGITS) for i in range(2 ** NUM_DIGITS)],\n",
    "        'y': [fizz_buzz_encode(i) for i in range(2 ** NUM_DIGITS)]\n",
    "    })\n",
    "\n",
    "class FizzbuzzDataset(data.Dataset):\n",
    "    def __init__(self, num_bits=NUM_DIGITS, mode='train'):\n",
    "        super(FizzbuzzDataset, self).__init__()\n",
    "        \n",
    "        if not DATA_CACHE:\n",
    "            fill_cache(num_bits)\n",
    "            \n",
    "        start, end = (0, 100) if mode == 'val' else (100, len(DATA_CACHE['y']))\n",
    "        self.idxs = list(range(start, end))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = DATA_CACHE['X'][self.idxs[idx]]\n",
    "        x = x.astype(np.float32)\n",
    "        y = DATA_CACHE['y'][self.idxs[idx]]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As said above, we create a simple model composed by a total of 3 feed forward layers, where the first one has `num_digits` inputs for each sample, followed by an activation fuction. We say our model must have 50 \"neural units\" for each one of the hidden layers and, given that we have 4 classes, an output dimension of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/docs/master/nn.html#torch.nn.LeakyReLU\n",
    "class FizzbuzzModel(nn.Module):\n",
    "    def __init__(self, h_dim=50, input_dim=NUM_DIGITS, num_classes=4):\n",
    "        super(FizzbuzzModel, self).__init__()\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, h_dim),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Sequential(\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(h_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SAME AS PREVIOUS BUT USING F (nn.functional)\n",
    "class FizzbuzzModel(nn.Module):\n",
    "    def __init__(self, h_dim=50, input_dim=NUM_DIGITS, num_classes=4):\n",
    "        super(FizzbuzzModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, h_dim)\n",
    "        self.linear2 = nn.Linear(h_dim, h_dim)\n",
    "        self.classifier = nn.Linear(h_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.linear1(x))\n",
    "        x = F.leaky_relu(self.linear2(x))\n",
    "        x = self.classifier(x)\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "let's instantiate our dataset objects, their corresponding dataloaders, our FizzbuzzModel, the usual SGD optimizing algorithm and a cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dataset_tr = FizzbuzzDataset(mode='train')\n",
    "dataset_val = FizzbuzzDataset(mode='val')\n",
    "dataloader_tr = data.DataLoader(dataset_tr, batch_size=128, shuffle=True)\n",
    "dataloader_val = data.DataLoader(dataset_val, batch_size=128, shuffle=False)\n",
    "\n",
    "model = FizzbuzzModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=.05, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "we thus train our model for 500 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.1389070749282837\n",
      "Epoch: 100, loss: 0.0060807508416473866\n",
      "Epoch: 200, loss: 0.003156117396429181\n",
      "Epoch: 300, loss: 0.000773159321397543\n",
      "Epoch: 400, loss: 0.00013801614113617688\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(500):\n",
    "    # train loop\n",
    "    for x, y in dataloader_tr:\n",
    "        x, y = Variable(x), Variable(y)\n",
    "        l = loss(model(x), y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    if not epoch % 100:\n",
    "        print('Epoch: {}, loss: {}'.format(epoch, l.data.numpy()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Finally we run the prediction on our evaluation set, which contains numbers from 0 to 99, we then fizz buzz encode the model's predictions and print the fizz buzz encoded values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  1.0 , Errors:  0\n",
      "['fizzbuzz', '1', '2', 'fizz', '4', 'buzz', 'fizz', '7', '8', 'fizz', 'buzz', '11', 'fizz', '13', '14', 'fizzbuzz', '16', '17', 'fizz', '19', 'buzz', 'fizz', '22', '23', 'fizz', 'buzz', '26', 'fizz', '28', '29', 'fizzbuzz', '31', '32', 'fizz', '34', 'buzz', 'fizz', '37', '38', 'fizz', 'buzz', '41', 'fizz', '43', '44', 'fizzbuzz', '46', '47', 'fizz', '49', 'buzz', 'fizz', '52', '53', 'fizz', 'buzz', '56', 'fizz', '58', '59', 'fizzbuzz', '61', '62', 'fizz', '64', 'buzz', 'fizz', '67', '68', 'fizz', 'buzz', '71', 'fizz', '73', '74', 'fizzbuzz', '76', '77', 'fizz', '79', 'buzz', 'fizz', '82', '83', 'fizz', 'buzz', '86', 'fizz', '88', '89', 'fizzbuzz', '91', '92', 'fizz', '94', 'buzz', 'fizz', '97', '98', 'fizz']\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "ys = []\n",
    "for x, y in dataloader_val:\n",
    "    x = Variable(x)\n",
    "    preds.extend(model(x).max(1)[1].data.tolist())\n",
    "    ys.extend(y)\n",
    "    \n",
    "correct = np.array(preds) == np.array(ys)\n",
    "predictions = zip(range(0, 100), preds)\n",
    "\n",
    "print('Accuracy: ', correct.mean(), ', Errors: ', np.logical_not(correct).sum())\n",
    "print ([fizz_buzz_decode(i, x) for (i, x) in predictions])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
