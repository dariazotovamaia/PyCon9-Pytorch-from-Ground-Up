{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will see now how to implement a bit more complex task with PyTorch. Which is, to predict the [*log-*]probability of a sample point (which could be a vector or a tensor) to belonging to a certain class, thus assigning the proper class label to each data point.\n",
    "\n",
    "To to this we could use a simple linaer layer as in the previous section, but a different loss, in particular we will use the cross entropy loss, which internally computes the softmax for the model's outputs (selecting the label with highest probability for each sample in a batch), NORMALIZZA TRA 0 E 1 (DA UN VETTORE CHE NON SOMMA A UN VETTORE CHE SOMMA A UNO IN MODO PROPORZIONALE) and the NLLLoss, or Negative Log Likelihood Loss. \n",
    "\n",
    "In information theory, the cross entropy between two probability distributions *p* and *q* over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set, if a coding scheme is used that is optimized for an \"unnatural\" probability distribution *q*, rather than the \"true\" distribution *p*.\n",
    "\n",
    "http://pytorch.org/docs/master/_modules/torch/nn/functional.html#cross_entropy\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "let's import our usual packages and add the magic function to see the plots in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To approach this task, we want to classify MINST images (hand written digits), which are 28x28 B/W images, which sees images having thus 28*28=784 values, so the input size for our starting layer will be 748, MNIST has 0-9 digits, so we will have 10 classes, the others are more free parameters and we can set them how we want, let's say that we want to train our model for 5 epochs making it see the data from 100 images at each step and applying the updates to the parameters with a learning rate of 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We then use again the super convenient dataset classes from the **torchvision** package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='../data/mnist', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='../data/mnist', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "as seen previously we can use pyplot to show us some of the images from the dataset, just be aware that pyplot normalize the values before showing them, if nothing else is specified. In this case the images are B/W and have 1 channel, so we could take the 0-th element of the tensor to actually take the image matrix and show it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28]), label: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131a65748>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = train_dataset[0]\n",
    "print('Shape: {},'.format(x.shape), 'label: {}'.format(y))\n",
    "plt.imshow(x.numpy()[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We build our model as usual, the parametric class definition is the same as the one in the previous section (apart from the class name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Sequential(nn.Linear(input_size, num_classes), nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "we than instantiate the model object, our loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "# Loss and Optimizer\n",
    "# Softmax is internally computed.\n",
    "# Set parameters to be updated.\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "we can now perform the training as before (as you can see thanks to pytorch we changed very few lines of code to literally change the task), given that the data points are a lot more than before, this training will take largely more time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [300/600], Loss: 2.2857\n",
      "Epoch: [1/5], Step: [600/600], Loss: 2.2647\n",
      "Epoch: [2/5], Step: [300/600], Loss: 2.2363\n",
      "Epoch: [2/5], Step: [600/600], Loss: 2.2316\n",
      "Epoch: [3/5], Step: [300/600], Loss: 2.2059\n",
      "Epoch: [3/5], Step: [600/600], Loss: 2.2051\n",
      "Epoch: [4/5], Step: [300/600], Loss: 2.1825\n",
      "Epoch: [4/5], Step: [600/600], Loss: 2.1874\n",
      "Epoch: [5/5], Step: [300/600], Loss: 2.1473\n",
      "Epoch: [5/5], Step: [600/600], Loss: 2.1460\n"
     ]
    }
   ],
   "source": [
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print ('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f' \n",
    "                   % (epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After we trained our model, we can see its performances by performing a loop over all the test images, storing the maximum model's output for each image (label's log probability) and comparing it to our ground thruth to get an accuracy metric. In this case we also use ConfusionMeter to show a confusion matrix for our model on this task.\n",
    "\n",
    "A confusion matrix permits to visualize the performances of a model on a classification task, having the ground thruths on its rows and the model's prediction on its columns, the values in this matrix'diagonal diagonal represent the numb er of correctly predicted samples, while the values on the other cells represents how many samples belonging to a class`c` get predicted as of another class `d`.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "preds_list = []\n",
    "labels_list = []\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28*28))\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    preds_list += [predicted.numpy()]\n",
    "    labels_list += [labels.numpy()]\n",
    "    correct += (predicted == labels).sum()\n",
    "preds_ary = np.hstack(preds_list)\n",
    "labels_ary = np.hstack(labels_list)\n",
    "confmat = confusion_matrix(preds_ary, labels_ary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "so we can now show our confuzion matrix and validation accuracy, and saving the model state_dit as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 68 %\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "plt.matshow(confmat)\n",
    "# Save the Model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
