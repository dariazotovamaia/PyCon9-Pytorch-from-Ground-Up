{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> PyTorch From the Ground Up\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<center> [L. Antiga](http://twitter.com/lantiga), [D. Ciriello](http://twitter.com/dnlcrl) and [A. Paszke](http://twitter.com/apaszke)\n",
    "\n",
    "<center> [PyCon Nove (2018)](https://pycon.it/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Before getting started\n",
    "\n",
    "Python3 is required to run this tutorial. You also will need some libraries from SciPy package (NumPy, Matplotlib, Pandas), Jupyter Notebook support, and Pytorch 0.4.0 or nightly.\n",
    "\n",
    "- create (a folder and once cd'ed it) an environtment for this training:\n",
    "\n",
    "    mkdir pycon9-pytorch\n",
    "    cd pycon9-pytorch\n",
    "    \n",
    "#### non-Conda\n",
    "\n",
    "    python3 -m venv pycon9-pytorch-env\n",
    "    source pycon9-pytorch-env/bin/activate\n",
    "    \n",
    "#### Conda\n",
    "\n",
    "    conda create --name pycon9-pytorch-env\n",
    "    source activate pycon9-pytorch-env  \n",
    "    (for windows: activate myenv)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "- to run jupyter notebooks:\n",
    "\n",
    "    python3 -m pip install jupyter\n",
    "\n",
    "\n",
    "- to run pytorch examples (torchnet and scikit-learn not essentials):\n",
    "\n",
    "\n",
    "    python3 -m pip install torch torchvision torchnet\n",
    "    python3 -m pip install numpy matplotlib PIL\n",
    "    \n",
    "@Conda users: replace `python3 -m pip` with `conda`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Before gettting started, we suggest you to start installing pytorch, some necessary packages and the needed data to use with the code we will run, so while we talk during this little presentation, is highly suggested that you run those lines in your terminal and install all needed packages for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Download Data for the Training\n",
    "\n",
    "During these hours we will make use of some light dataset, if you don't download the data now it, theoretically it will be downloaded automatically later, but if you want to download the data in advance you can use one of the following links:\n",
    "\n",
    "- https://bit.ly/2qstREA (DropBox)\n",
    "- https://bit.ly/2EI4NOY (GDrive, and it's an upper i)\n",
    "- https://bit.ly/2vdLLAF (iCloud)\n",
    "- https://goo.gl/uHm37T (OneDrive)\n",
    "\n",
    "you should get a `data.zip` archive (380MB), unzip it and put the unzipped `data` directory in your `pycon9-pytorch` folder.\n",
    "\n",
    "You should then be ready for the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "while the packages installation is pretty propedeutics, you can skip the data downloading step, but still is highly suggested that you start to download the data because nobody knows what could happens later, if you want to, we also have some usb stick with the data in it, so let us know if you have problems.\n",
    "\n",
    "Windows warning: we prepared this training on a Unix machine and we are pretty cinfident it will work also on Lunix, but we haven't tested it on a windows machine, so, again, let us know if you have problems and we will try to fix it togheter (or you can simply fix it by yourself or avoid using a window machine ;P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This trainning shows you how to train a deep neural network using [PyTorch](http://pytorch.org/). PyTorch is a Python package aimed at accelerating deep learning applications. PyTorch provides a [Numpy](http://www.numpy.org/)-like abstraction for representing _tensors_, or multidimensional arrays, and it can take advantage of [GPUs](http://www.nvidia.com/object/what-is-gpu-computing.html) for performance. The tutorial ends with some case study, which is how to use PyTorch for performing classification tasks and some other intresting things.\n",
    "\n",
    "> Besides PyTorch, there are numerous proposed tools and extensions to get GPU acceleration for multiway arrays. See **Suggested Next Steps** at the end of this tutorial for pointers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Knowledge prerequisites.** This tutorial assumes familiarity with Python and Numpy, for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "From there, PyTorch makes it easy to get started with deep learning even if your background in the topic is modest. At a minimum, it's helpful to know that a multilayer neural network model may be viewed as a graph of nodes (values and functions on values) connected by weights (the unknown parameters of the model), where one wishes to estimate the weights from data using an optimization procedure (think: gradient computations!) based on forward and backward propagation.\n",
    "\n",
    "**Software prerequisites.** You'll need to install PyTorch before running this notebook. Here's a code cell to verify that you are ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Hardware niceties.** Lastly, to exploit GPUs, you'll need an NVIDIA GPU with the CUDA SDK installed. It is reported that 10-100$\\times$ speedups are possible by doing so. Of course, if you don't have such a setup, PyTorch still can run using the CPU only. But remember, when it comes to training neural network models, life is short---so you should use GPUs if you can!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **Presentation Outline.**\n",
    "\n",
    "### 1. Essential PyTorch Background\n",
    "### 2. Training Machines with PyTorch\n",
    "### 3. Real World Applications\n",
    "### 4. Training with PyTorch Basics\n",
    "### 5. Examples Hands-on\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **1. Essential PyTorch Background**\n",
    "\n",
    "#### 1.1 PyTorch, Torch, Numpy\n",
    "#### 1.2 Tensors\n",
    "#### 1.3 Tensors VS Numpy\n",
    "#### 1.4 Variables\n",
    "#### 1.5 Computation Graph\n",
    "#### 1.6 Backpropagation with PyTorch\n",
    "#### 1.7 Cuda Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Essential PyTorch background\n",
    "\n",
    "* PyTorch is a Python package aimed at accelerating deep learning applications. \n",
    "* PyTorch provides a [Numpy](http://www.numpy.org/)-like abstraction for representing _tensors_, or multidimensional arrays, and it can take advantage of [GPUs](http://www.nvidia.com/object/what-is-gpu-computing.html) for performance.\n",
    "\n",
    "<img src=\"images/tensor.png\" style=\"max-width:100%; width: 75%; max-width: none\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.1 PyTorch, Torch, Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "pytorch comes from its predecessor Lua torch, written coincidentally in Lua. Which aimed to be a xxx with anologues function to numpy and its core written in C, etc. Why Python instead of lua and why numpy as top aims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 PyTorch Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The key data abstraction of PyTorch is a tensor, which is a multidimensional array. It is similar in functionality to Numpy's `ndarray` object. Use [torch.tensor()](http://pytorch.org/docs/master/tensors.html) to create one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a 2-D pytorch tensor (i.e., a matrix)\n",
    "tensor = torch.tensor([[10, 20], [30, 40]])\n",
    "print(\"tensor: \", tensor)\n",
    "print(\"type: \", type(tensor), \" and size: \", tensor.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "If you need a Numpy-compatible representation, or if you want to create a PyTorch tensor from an existing Numpy object, it's easy to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the pytorch tensor to a numpy array:\n",
    "arr = tensor.numpy()\n",
    "print(\"type: \", type(arr), \" and size: \", arr.shape)\n",
    "\n",
    "# Convert the numpy array to Pytorch Tensor:\n",
    "tensor2 = torch.from_numpy(arr)\n",
    "print(\"type: \", tensor2.dtype, \" and size: \", tensor2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 PyTorch vs. NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "PyTorch is not a drop in replacement for NumPy, but it implements a lot of Numpy functionality. One inconvenience is it's naming scheme that sometimes is rather different from Numpy. Let's go over several examples to see the difference. Let's look at PyTorch and NumPy differences on various examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 1. Tensor creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.rand(2, 4, 3, 5)\n",
    "a = np.random.rand(2, 4, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Random Tensor with size (3, 4):\\n', torch.rand(3, 4))\n",
    "print(\"Random NdArray with size (3, 4):\\n\", np.random.rand(3, 4))\n",
    "print('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You can see other initializations schemes like zeros, ones, and identity matrices below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print('Tensor of zeros with size (3, 4):\\n', torch.zeros(3, 4))\n",
    "print('NdArray of zeros with size (3, 4):\\n', np.zeros((3, 4)))\n",
    "print('-------------------------------------------------------------------------')\n",
    "\n",
    "print('Tensor of ones with size (3, 4):\\n', torch.ones(3, 4))\n",
    "print('NdArray of ones with size (3, 4):\\n', np.ones((3, 4)))\n",
    "print('-------------------------------------------------------------------------')\n",
    "\n",
    "print('Identity Tensor with size (3, 4):\\n', torch.eye(3, 4))\n",
    "print(\"Identity NdArray with size (3, 3) (can't create non-squared identity matrix):\\n\", np.eye(3))\n",
    "print('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 2. Tensor slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "t = torch.rand(2, 4, 3, 5)\n",
    "a = t.numpy()\n",
    "\n",
    "print ('Tensor:\\n', t)\n",
    "print ('NdArray:\\n', a)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pytorch_slice = t[0, 1:3, :, 4]\n",
    "numpy_slice =  a[0, 1:3, :, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor[0, 1:3, :, 4]:\\n', pytorch_slice)\n",
    "print ('NdArray[0, 1:3, :, 4]:\\n', numpy_slice)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Below you can find more slicing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor size:\\n', t.size())\n",
    "print ('NdArray size:\\n', a.shape)\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Tensor[0][1][2][3]:\\n', t[0][1][2][3])\n",
    "print ('NdArray[0][1][2][3]:\\n', a[0][1][2][3])\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Tensor[0, 1, 2, 3]:\\n', t[0, 1, 2, 3])\n",
    "print ('NdArray[0, 1, 2, 3]:\\n', a[0, 1, 2, 3])\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Tensor[0][1]:\\n', t[0][1])\n",
    "print ('NdArray[0][1]:\\n', a[0][1])\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Tensor[0, 1:3]:\\n', t[0, 1:3])\n",
    "print ('NdArray[0, 1:3]:\\n', a[0, 1:3])\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 3. Tensor masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "t = t - 0.5\n",
    "a = t.numpy()\n",
    "\n",
    "print ('Tensor:\\n', t)\n",
    "print ('NdArray:\\n', a)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pytorch_masked = t[t > 0]\n",
    "numpy_masked = a[a > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor[Tensor > 0]:\\n', pytorch_masked)\n",
    "print ('NdArray[NdArray > 0]:\\n', numpy_masked)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Below you can see how conditioning works with PyTorch, and how array size is changed after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor > 0:\\n', t > 0)\n",
    "print ('NdArray > 0:\\n', a > 0)\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Tensor[Tensor > 0]:\\n', t[t > 0])\n",
    "print ('NdArray[NdArray > 0]:\\n', a[a > 0])\n",
    "print ('-------------------------------------------------------------------------')\n",
    "\n",
    "print ('Size of Tensor[Tensor > 0]:\\n', t[t > 0].size())\n",
    "print ('Size of NdArray[NdArray > 0]:\\n', a[a > 0].shape)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### 4. Tensor reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor:\\n', t)\n",
    "print ('NdArray:\\n', a)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pytorch_reshape = t.view([6, 5, 4])\n",
    "numpy_reshape = a.reshape([6, 5, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print ('Tensor reshaped to (6:5:4):\\n', pytorch_reshape)\n",
    "print ('NdArray reshaped to (6:5:4):\\n',numpy_reshape)\n",
    "print ('-------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "You can also see a permutation example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print ('Tensor with dimensions transposed as (1, 0, 2):\\n', pytorch_reshape.permute(1, 0, 2) )\n",
    "print ('NdArray with dimensions transposed as (1, 0, 2):\\n', np.transpose(numpy_reshape, (1, 0, 2)))\n",
    "print ('-------------------------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.4 Tensors know how to autograd\n",
    "\n",
    "* Gradients with respect to a target variable are kept in **.grad**.\n",
    "* Used to be Variable in PyTorch < 0.4\n",
    "<img src=\"images/variable.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.4 Tensors know how to autograd\n",
    "\n",
    "* Builds **computation graph** dynamically\n",
    "* **.backward()** on a variable to compute its derivatives wrt variables\n",
    " - if the variable is not scalar, you need to specify a grad_output arg (tensor of matching shape)\n",
    "* **.grad_fn** references a function that has created a tensor (except for tensors created by the user) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print(x.grad)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "z = y ** 2 * 3\n",
    "out = z.mean()\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.5 Computation Graph\n",
    "* DAG in which nodes represents variables and edges represent mathematical operations\n",
    "* Pruned where possible (removing edges/nodes)\n",
    "* The graph starts with leafs only \n",
    "* Root variables computed during backward pass\n",
    "* Derivatives are computed by traversing the graph and accumulating gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In simple terms, a computation graph is a DAG in which nodes represent variables (tensors, matrix, scalars, etc.) and edge represent some mathematical operations (for example, summation, multiplication).\n",
    "\n",
    "The computation graph has some leaf variables. The root variables of the graph are computed according to operations defined by the graph. During the optimization step, we combine the chain rule and the graph to compute the derivative of the output w.r.t the learnable variable in the graph and update these variables to make the output close to what we want. In neural networks, learnable variables are for instance weight and bias in a linear module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "**Computation graphs and variables.** In PyTorch, a neural network is represented by a _computation graph_ composed of interconnected _variables_. PyTorch allows you to build the network model by constructing this graph in code; it then simplifies the process of estimating the weights of this model by, for example, automatically calculating gradients.\n",
    "\n",
    "For example, suppose we wish to build the following two-layer model. Let's start by creating  tensor inputs $x$ and outputs $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.randn(5, 1, requires_grad=False)\n",
    "y = torch.randn(5, 1, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We set requires_grad to True to say that we want the gradient to be computed automatically which will be used in backpropagation to optimize the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we define the weights: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "w1 = torch.randn(7, 5, requires_grad=True)\n",
    "w2 = torch.randn(5, 7, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def model_forward(x):\n",
    "    return F.sigmoid(w2.mm(F.sigmoid(w1.mm(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_forward(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print (w1)\n",
    "print (w1.data.shape)\n",
    "print (w1.grad) # Initially, non-existent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.6 Backpropagation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So, we have inputs and targets, we have our simple model's weights, now it's time to train them. And for this, we need three components: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Loss** that describes how far our model from the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Optimization algorithm** that we use to update the weight update, and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD([w1, w2], lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Backpropagation step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    loss = criterion(model_forward(x), y)\n",
    "    optimizer.zero_grad() # Zero-out previous gradients\n",
    "    loss.backward() # Compute new gradients\n",
    "    optimizer.step() # Apply these gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "print (w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  1.7 CUDA interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One of the benefits of PyTorch is that it provides a CUDA interface for its tensor and autograd libraries. Using CUDA GPGPUs you can accelerate not onlu neural network training and inference, but also any other workload that maps to PyTorch tensors.\n",
    "\n",
    "You can check whether you have CUDA available in PyTorch by calling **torch.cuda.is_available()** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cuda_gpu = torch.cuda.is_available()\n",
    "if (cuda_gpu):\n",
    "    print(\"Great, you have a GPU!\")\n",
    "else:\n",
    "    print(\"Life is short -- consider a GPU!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## .cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Spoiler**: if you don't have a cuda GPU, or it's not configured for PyTorch, the next two cells are going to fail without wrapping them into if statement or in try block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After that, accelerating you code with cuda is as easy as calling **.cuda()** on your tensors and models. If you call **.cuda()** on tensors, it will perform data transfer from CPU to CUDA GPU. If you call **.cuda()** on a model, it not only moves all its internal storage to GPU, but also maps the whole computational graph to GPU.\n",
    "\n",
    "To copy a tensor or a model back to CPU, for example in order to interface it with NumPy, you can call **.cpu()**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "if cuda_gpu:\n",
    "    x = x.cuda()\n",
    "    print(x)\n",
    "\n",
    "x = x.cpu()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "if cuda_gpu:\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    w1 = w1.cuda()\n",
    "    w2 = w2.cuda()\n",
    "\n",
    "print (x)\n",
    "for epoch in range(10):\n",
    "    loss = criterion(model_forward(x), y)\n",
    "        \n",
    "    optimizer.zero_grad() # Zero-out previous gradients\n",
    "    loss.backward() # Compute new gradients\n",
    "    optimizer.step() # Apply these gradients\n",
    "print (w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, epoch, criterion, optimizer, data_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        if cuda_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            model.cuda()\n",
    "        output = model(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (batch_idx+1) % 400 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx+1) * len(data), len(data_loader.dataset),\n",
    "                100. * (batch_idx+1) / len(data_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def test(model, epoch, criterion, data_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    for data, target in data_loader:\n",
    "        if cuda_gpu:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            model.cuda()\n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss /= len(data_loader) # loss function already averages over batch size\n",
    "    acc = correct / len(data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(data_loader.dataset), 100. * acc))\n",
    "    return (acc, test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let's define two functions, train and test, to perform training and inference using our model. This code is also adopted from the PyTorch official tutorial and shows all the steps necessary for training/inference. For the training and testing aour network we need to perform a sequence of actions, that are mapped to PyTorch code fairly straightforward:\n",
    "\n",
    "1. We switch model to training/inference mode\n",
    "2. We iterate over the dataset fetching images in batches\n",
    "3. For every batch we load data and targets and running forward step of the network to get the model outputs\n",
    "4.  We define a loss function and compute loss between model outputs and targets on per batch basis\n",
    "5. In case of training, we zero gradients and use backpropagation with optimizer defined on the previous step to compute gradients of all the layers with respect to loss.\n",
    "6. In case of training, we perform a weight update step\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After this introduction, we can start our data science journey ! The rest of the tutorial is loosely based on these [PyTorch examples](https://github.com/pytorch/examples/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Training Machines with PyTorch\n",
    "\n",
    "#### 2.1 Build Models with **torch.nn**\n",
    "#### 2.2 Manage your Data with **torch.utils.data**\n",
    "#### 2.3 Optimize the model's parameters with **torch.optim**\n",
    "#### 2.3 Compute Gradients Automatically with **torch.autograd**\n",
    "#### 2.4 Return to Numpy's ndarrays with **.numpy()** interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Before we look at more complex models, let's start with something really simple - linear regression and synthetic toy dataset that we can generate with sklearn kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()\n",
    "\n",
    "x_train, y_train, W_target = make_regression(n_samples=100, n_features=1, noise=10, coef = True)\n",
    "\n",
    "df = pd.DataFrame(data = {'X':x_train.ravel(), 'Y':y_train.ravel()})\n",
    "\n",
    "sns.lmplot(x='X', y='Y', data=df, fit_reg=True)\n",
    "plt.show()\n",
    "\n",
    "x_torch = torch.tensor(x_train)\n",
    "y_torch = torch.tensor(y_train)\n",
    "y_torch = y_torch.view(y_torch.size()[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.1 Build Models with **torch.nn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Simplest Model, single linear layer with no activation function\n",
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressor, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_size, output_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearRegressor(1, 1)\n",
    "\n",
    "# equal to:\n",
    "#   model = nn.Linear(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **nn** = neural network module\n",
    "* Main PyTorch's module providing neural networks\n",
    "* Modules:\n",
    " * implementations of neural networks building blocks\n",
    " * custom models should subclass **torch.nn.Module**\n",
    " * automatic conversions when calling **.cuda()**\n",
    " * **forward()** method must be implemented\n",
    " * propagation of **.train()** and **.eval()** modes\n",
    " * .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "PyTorch has a lot of useful modules in its **nn** library. One of them is linear. As the name suggests, it performs a linear transformation of its input, which is essentially linear regression does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To train a linear regression, we will need to add the right loss function from the same **nn** library. For linear regression we will use **MSELoss()**, mean squared error loss function.\n",
    "\n",
    "We will also need to use optimization function (SGD), and run a backpropagation similar to our previous toy example. Essentially, we repeating the steps from the **train()** function we defined above. The reason we can't use this function directly is that we implemented it for classification, not for regression, and as model prediction we use the index of the maximum element of cross-entropy loss. For linear regression we use output of linear layer as a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  \n",
    "\n",
    "for epoch in range(50):\n",
    "    data, target = x_torch.to(torch.float), y_torch.to(torch.float)\n",
    "    output = model(data)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "        \n",
    "predicted = model(x_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We can now print the original data and linear regression that we fit with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'o', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Manage your Data with **torch.utils.data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **torch.utils.data.Dataset** Abstract Class\n",
    " - subclasses must implement **\\__len__(self)** and **\\__getitem__(self, idx)** functions\n",
    "- **torch.utils.data.DataLoader** Dataset iterator\n",
    " - iterates Dataset's items and stack them into tensors\n",
    "- **torch.utils.data.Sampler** Dataset index sampler\n",
    " - generates **idx** for each **\\__getitem__** call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To move forward with more complex models, let's download MNIST dataset to your 'datasets' folder and test some initial pre-processing that's available in PyTorch. PyTorch has dataloaders and handlers for various datasets. Once downloaded, you can use them any time. You can also wrap your data in PyTorch tensors and create your own data loader class.\n",
    "\n",
    "Batch size is a term used in machine learning and refers to the number of training examples utilised in one iteration. The batch size can be one of three options:\n",
    "-  batch mode: where the batch size is equal to the total dataset thus making the iteration and epoch values equivalent\n",
    "-  mini-batch mode: where the batch size is greater than one but less than the total dataset size. Usually, a number that can be divided into the total dataset size.\n",
    "-  stochastic mode: where the batch size is equal to one. Therefore the gradient and the neural network parameters are updated after each sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2.1 **torch.utils.data.Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils import data\n",
    "\n",
    "class RegressionDataset(data.Dataset):\n",
    "    def __init__(self, n_samples=100, n_fearues=1, noise=10, coef=True):\n",
    "        super(RegressionDataset, self).__init__()\n",
    "        self.x_train, self.y_train, W_target = make_regression(\n",
    "            n_samples=n_samples, \n",
    "            n_features=n_fearues, \n",
    "            noise=noise, \n",
    "            coef=coef)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "\n",
    "    def __getitem(self, idx):\n",
    "        return self.x_train[idx], self.y_train[idx]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "regression_dataset = RegressionDataset()\n",
    "plt.plot(regression_dataset.x_train, regression_dataset.y_train, 'o', label='Original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2.2 **torch.utils.data.Sampler**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Base Class for all Samplers\n",
    "- subclasses must implement **\\__len__** and **\\__iter__** methods\n",
    "- provides a way to iterate over dataset elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2.2 **torch.utils.data.Sampler**\n",
    "Several basic implementations are provided by pytorch:\n",
    "- SequentialSampler\n",
    "- RandomSampler\n",
    "- SubsetRandomSampler\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Its subclasses must implement an **\\__iter__** method, providing a way to iterate over indexes of dataset elements, and a __len__ method that returns the lenght of the returned iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2.3 **torch.utils.data.DataLoader**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Combines a dataset and a sampler\n",
    "- provides single- or multi-process iterators\n",
    "- by default (and if **sampler=None**) makes uses of a **SequentialSampler** or a **RandomSampler** (depending on the **shuffle** parameter value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.3 Torch.optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- implements various optimization algorithms\n",
    "- **SGD**, **Adam**, etc\n",
    "- updates the model's parameters by calling its **.step()** method\n",
    "- permits LR management through **torch.optim.lr_scheduler** classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.1 Torch.optim eaxmple, **SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.3.1 Torch.optim.lr_scheduler eaxmple, StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=200, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- we increase the step value by calling its **step()** method\n",
    "- when it reach **step_size** calls, **optimizer.lr *= gamma** gets executed and **step_size** gets reinizialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.4 Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- provides automatic differentiation within tensors\n",
    "- only need to declare Tensors for which the gradient should be computed (using `requires_grad=True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.5 Back to numpy with **.numpy()** interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can always get back to numpy's ndarrays\n",
    "- **.numpy()** if on a tensor\n",
    "- **.data.numpy()** if on a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 3. Some Real World (CV) application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### What Does Orobix Do?\n",
    "Among other things:\n",
    "- Medical Imaging\n",
    "  - Organs segmentation\n",
    "  - Disease Classification\n",
    "- Industrial Analysis\n",
    "  - Prototypical Learning\n",
    "  - VaGans\n",
    "- Gaming / Robot control\n",
    "  - Reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Organs segmentation\n",
    "    Schermata 2018-04-09 alle 16.00.31\n",
    "    \n",
    "    \n",
    "Disease Classification\n",
    "    Schermata 2018-04-09 alle 15.59.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Prototypical Learning\n",
    "    substitution for T-Sne (we let the model learns the samples embeddings in its latent space)\n",
    "    \n",
    "![img from paper](https://raw.githubusercontent.com/orobix/Prototypical-Networks-for-Few-shot-Learning-PyTorch/master/doc/imgs/proto-1.png)\n",
    "    \n",
    "We used TSNE before and now this method to let people understand what the model is doing and why a certain sample undergoes a certain classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "VAGAN (reference to vagan code)\n",
    "Visual Feature Attribution with Wasserstein GANs\n",
    "[img][reference]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Reinforcement Learning to teach agents to think\n",
    "[img/vid/vid link]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Training with PyTorch. Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Basics\n",
    "- Linear Regression\n",
    "- Logistinc Regression\n",
    "- Exercise: FizzBuzz with PyTorch\n",
    "- MNIST Intro\n",
    "- Linear Regression MNIST\n",
    "- Logistic Regression MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. Training with PyTorch. Intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- torchvision package\n",
    "- Convolutions\n",
    "- Classifying MNIST with a convolutional nnet\n",
    "- pretrained model's predictions\n",
    "- transfer learning\n",
    "- fine tuning\n",
    "- CharRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
